{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the English vocabulary of the EP full-text data for text analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antoine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# librairies\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# library to parse the xml content of the EP full text database\n",
    "# library doc: https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# language processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# location the the files - EP full text data 2020 edition\n",
    "fpattern = r'../data/ep_full_text_database/2020_edition/EP{}.txt'\n",
    "files = glob.glob(fpattern.format('*','*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# config\n",
    "sep = '\\t'\n",
    "text_type = 'CLAIM'\n",
    "lang = 'en'\n",
    "\n",
    "new_col_names = ['publication_authority', # will always have the value \"EP\"\n",
    "                 'publication_number', # a seven-digit number\n",
    "                 'publication_kind', # see https://www.epo.org/searching-for-patents/helpful-resources/first-time-here/definitions.html for help.\n",
    "                 'publication_date', # in format YYYY-MM-DD\n",
    "                 'language_text_component', # de, en, fr; xx means unknown\n",
    "                 'text_type', # TITLE, ABSTR, DESCR, CLAIM, AMEND, ACSTM, SREPT, PDFEP\n",
    "                 'text' # it contains, where appropriate, XML tags for better structure. You will find the DTD applicable to all parts of the publication at: http://docs.epoline.org/ebd/doc/ep-patent-document-v1-5.dtd\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonction to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_claim_text(f):\n",
    "    \"\"\"Open the file f to get all the claims in English\"\"\"\n",
    "    \n",
    "    # reading the file \n",
    "    print('Reading the file: {}'.format(f))\n",
    "    df = pd.read_csv(f, sep = sep)\n",
    "    \n",
    "    # changing the column names\n",
    "    df.columns = new_col_names\n",
    "    \n",
    "    # filtering to keep only claims in English (and only once)\n",
    "    condition1 = df['text_type'] == text_type\n",
    "    condition2 = df['language_text_component'] == lang\n",
    "    df.drop_duplicates(subset = ['text'], inplace = True)\n",
    "    df = df[condition1 & condition2]['text'].to_frame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parsing(text_xml):\n",
    "    \"\"\"Process the xml to get the raw text\"\"\"\n",
    "            \n",
    "    # removing the tags for bold text\n",
    "    text_xml_modified = text_xml.replace('<b>', '')\n",
    "    text_xml_modified = text_xml_modified.replace('</b>', '')\n",
    "\n",
    "    # modifying the claim to be processed as a real xml\n",
    "    text_xml_modified = \"<data>\" + text_xml_modified + '</data>'\n",
    "    # we parse it with the ElementTree XML APIÂ¶\n",
    "    root = ET.fromstring(text_xml_modified)\n",
    "    # and this is how we access the text of the claims\n",
    "    claims = root.findall(\"./claim/claim-text\")\n",
    "    # we store the claims in a list\n",
    "    claims_text = [claim.text for claim in claims]\n",
    "    \n",
    "    return claims_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2400000.txt\n",
      "CPU times: user 1min 13s, sys: 6.27 s, total: 1min 20s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test with a single file\n",
    "df = get_claim_text(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fetch_all_vocabulary(files):\n",
    "    l = []\n",
    "    for f in files:\n",
    "        text = get_claim_text(f)\n",
    "        text['text'].apply(parsing)\n",
    "        l.append(df)\n",
    "    return pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2100000.txt\n",
      "CPU times: user 47min 25s, sys: 1min 49s, total: 49min 14s\n",
      "Wall time: 1h 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = fetch_all_vocabulary(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# reshape \n",
    "documents = df['text'].apply(parsing)\n",
    "documents = documents.apply(lambda x:x[0])\n",
    "documents.dropna(inplace=True)\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer(\"(?u)\\\\b[\\\\w-]+\\\\b\")\n",
    "documents = documents.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     4,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def remove_capitalisation(x):\n",
    "    liste = [y.lower() for y in x]\n",
    "    return liste\n",
    "\n",
    "def remove_stop_words(x):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    liste = [y for y in x if not y in stopset]\n",
    "    return liste\n",
    "\n",
    "def remove_numbers(x):\n",
    "    liste = [y for y in x if not any(char.isdigit() for char in y)]\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents.apply(remove_capitalisation)\n",
    "documents = documents.apply(remove_stop_words)\n",
    "documents = documents.apply(remove_numbers)\n",
    "documents = documents.apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comprising', 2947901),\n",
       " ('said', 1222110),\n",
       " ('method', 1194397),\n",
       " ('least', 980722),\n",
       " ('one', 975246),\n",
       " ('device', 955673),\n",
       " ('wherein', 917563),\n",
       " ('first', 670773),\n",
       " ('system', 587375),\n",
       " ('second', 514596),\n",
       " ('apparatus', 421837),\n",
       " ('means', 409146),\n",
       " ('comprises', 403596),\n",
       " ('element', 382321),\n",
       " ('characterized', 370962),\n",
       " ('surface', 340067),\n",
       " ('unit', 293373),\n",
       " ('body', 289044),\n",
       " ('two', 282643),\n",
       " ('part', 279239),\n",
       " ('arranged', 277833),\n",
       " ('control', 276945),\n",
       " ('material', 266844),\n",
       " ('end', 263033),\n",
       " ('connected', 255522),\n",
       " ('side', 250749),\n",
       " ('position', 243978),\n",
       " ('portion', 243460),\n",
       " ('plurality', 242942),\n",
       " ('provided', 238761),\n",
       " ('direction', 225774),\n",
       " ('vehicle', 224294),\n",
       " ('layer', 217856),\n",
       " ('composition', 211344),\n",
       " ('steps', 211196),\n",
       " ('including', 186591),\n",
       " ('base', 184593),\n",
       " ('housing', 183298),\n",
       " ('power', 181411),\n",
       " ('communication', 178821),\n",
       " ('formed', 177082),\n",
       " ('network', 173197),\n",
       " ('data', 171643),\n",
       " ('member', 163873),\n",
       " ('gas', 161098),\n",
       " ('process', 160432),\n",
       " ('use', 160099),\n",
       " ('structure', 157694),\n",
       " ('assembly', 157028),\n",
       " ('support', 156066),\n",
       " ('compound', 154216),\n",
       " ('axis', 151071),\n",
       " ('characterised', 150923),\n",
       " ('valve', 150664),\n",
       " ('signal', 144226),\n",
       " ('light', 144115),\n",
       " ('air', 143819),\n",
       " ('wall', 141451),\n",
       " ('formula', 140711),\n",
       " ('opening', 139860),\n",
       " ('outer', 135864),\n",
       " ('circuit', 133570),\n",
       " ('component', 133052),\n",
       " ('image', 132719),\n",
       " ('acid', 132645),\n",
       " ('region', 131609),\n",
       " ('fluid', 131313),\n",
       " ('machine', 129759),\n",
       " ('flow', 129463),\n",
       " ('frame', 127465),\n",
       " ('following', 126503),\n",
       " ('processing', 125837),\n",
       " ('shaft', 124912),\n",
       " ('motor', 124431),\n",
       " ('chamber', 123728),\n",
       " ('section', 122100),\n",
       " ('pressure', 121804),\n",
       " ('liquid', 121730),\n",
       " ('inner', 121249),\n",
       " ('plate', 120213),\n",
       " ('thereof', 118918),\n",
       " ('water', 117623),\n",
       " ('drive', 116846),\n",
       " ('connection', 116661),\n",
       " ('elements', 115958),\n",
       " ('optical', 113035),\n",
       " ('form', 112924),\n",
       " ('cell', 110667),\n",
       " ('configured', 109224),\n",
       " ('contact', 108262),\n",
       " ('transmission', 107004),\n",
       " ('mobile', 106819),\n",
       " ('particular', 105783),\n",
       " ('electric', 105450),\n",
       " ('container', 105080),\n",
       " ('upper', 104747),\n",
       " ('sensor', 103896),\n",
       " ('longitudinal', 103156),\n",
       " ('engine', 103008),\n",
       " ('information', 102046)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the most frequent words in the dataset\n",
    "Counter(\" \".join(documents).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
