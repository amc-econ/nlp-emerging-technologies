{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/antoine/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/antoine/.local/lib/python3.6/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/antoine/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/antoine/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/antoine/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.6 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import igraph\n",
    "from igraph import Graph\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library to parse the xml content of the EP full text database\n",
    "# library doc: https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "import xml.etree.ElementTree as ET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration\"\"\"\n",
    "    \n",
    "    # Magic numbers\n",
    "    LAST_YEAR_TO_RECEIVE_CITAITONS = 2018\n",
    "    PERCENTAGE_TOP_PATENTS = 0.01\n",
    "\n",
    "    # PASTAT_variables \n",
    "    VAR_APPLN_ID = 'appln_id'\n",
    "    VAR_DOCDC_FAMILY_ID = 'docdb_family_id'\n",
    "    VAR_CITED_DOCDB_FAM_ID = 'cited_docdb_family_id'\n",
    "    VAR_APPLN_FILLING_YEAR = 'appln_filing_year'\n",
    "    VAR_NB_CITING_DOCDB_FAM = 'nb_citing_docdb_fam'\n",
    "    VAR_EARLIEST_FILLING_DATE = 'earliest_filing_date'\n",
    "    VAR_EARLIEST_FILING_YEAR = 'earliest_filing_year'\n",
    "\n",
    "    # Computed variables\n",
    "    NEW_VAR_CITING_DOCDB_FAM_IDS = 'citing_docdb_families_ids'\n",
    "    NEW_VAR_NB_CITING_DOCDB_FAM_BY_YEAR = 'nb_citing_docdb_fam_by_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATSTAT data and EP Full-text data\n",
    "* We load the PATSTAT data previously retrieved with the `data_extraction_from_PATSTAT.ipynb` notebook\n",
    "* We load the data containing the full-text data for the patents of interest, extracted via the notebook `data_retrieval_from_EP_full_text_database.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the PATSTAT data previously retrieved with the data_extraction_from_PATSTAT.ipynb notebook\n",
    "output_files_prefix = \"wind_tech_1990_2020_with_publications\"\n",
    "pre = '../data/raw/' + output_files_prefix\n",
    "suf = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we store all the data retrieved into a single data object.\n",
    "data = {'_table_main_patent_infos': pd.read_csv(pre + '_table_main_patent_infos' + suf, low_memory=False),\n",
    "       '_table_cpc': pd.read_csv(pre + '_table_cpc' + suf, low_memory=False), \n",
    "       '_table_patentees_info': pd.read_csv(pre + '_table_patentees_info' + suf, low_memory=False),\n",
    "       '_table_backward_docdb_citations': pd.read_csv(pre + '_table_backward_docdb_citations' + suf, low_memory=False),\n",
    "       '_table_forward_docdb_citations': pd.read_csv(pre + '_table_forward_docdb_citations' + suf, low_memory=False),\n",
    "       '_text_data':pd.read_csv('../data/raw/wind_tech_1990_2020_with_publications_full_text.csv', sep = ',')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    \"\"\"Data cleaning methods\"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _correct_JP_data(self):\n",
    "        \"\"\"Correction for Japanese patent data, in line with the literature\"\"\"\n",
    "        # Do # Update the list of ids\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _keep_only_EP_patents(self):\n",
    "        \"\"\"We filter the data to keep only EU patents (not only EP)\"\"\"\n",
    "        \n",
    "        # Local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        condition = df_main[''].isin(Config.EU_authorities)\n",
    "        df_main = df_main[condition]\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _normalise(self):\n",
    "        \"\"\"Normalisation of the data accross years and sectors, to cater for **patent explosion**\"\"\"\n",
    "        # Do # Update the list of ids\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _select_one_patent_per_family(self):\n",
    "        \"\"\"In order to select only patent of interest, as well as\n",
    "        saving computationnal power, we select only the earliest patent by\n",
    "        family\"\"\"\n",
    "        \n",
    "        # Local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        df_cpc = self.data['_table_cpc']\n",
    "        df_patentees = self.data['_table_patentees_info']\n",
    "        \n",
    "        # Filtering \n",
    "        df_main.sort_values(by = Config.VAR_EARLIEST_FILLING_DATE,inplace = True)\n",
    "        df_main.drop_duplicates(subset = [Config.VAR_DOCDC_FAMILY_ID],\n",
    "                                keep = 'first',\n",
    "                                inplace = True)\n",
    "        \n",
    "        # Storing ids and filtering datasets\n",
    "        self = self.__update_patent_fam_ids()   \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def _select_breakthrough_patents(self):\n",
    "        \"\"\"Filtering the data to keep only breakthrough patents\"\"\"\n",
    "        \n",
    "        # Unpacking some variables for clarity\n",
    "        X = Config.PERCENTAGE_TOP_PATENTS\n",
    "        df = self.data['_table_main_patent_infos']\n",
    "        \n",
    "        # Selection  of the top patents\n",
    "        filtered_df = pd.DataFrame()\n",
    "        for year in df[Config.VAR_EARLIEST_FILING_YEAR].unique().tolist():\n",
    "            df_year = df[df[Config.VAR_EARLIEST_FILING_YEAR] == year]\n",
    "            df_year.sort_values(by = Config.VAR_NB_CITING_DOCDB_FAM,\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "            nb_top_patent_given_year = int(math.ceil(X*len(df_year))) # Needs rounding up\n",
    "            df_year = df_year.head(nb_top_patent_given_year)\n",
    "            filtered_df = pd.concat([filtered_df, df_year])\n",
    "            \n",
    "        # Update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = filtered_df\n",
    "        \n",
    "        # Storing ids and filtering datasets\n",
    "        self = self.__update_patent_fam_ids()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __update_patent_fam_ids(self):\n",
    "        \"\"\"\n",
    "        Storing patents ids and family ids and filtering the datasets\n",
    "        # Filtering the first 3 datasets on the list of patent ids \n",
    "        # Filtering the other 2 datasets on the list of family ids\n",
    "        \"\"\"\n",
    "        \n",
    "        # (1) Update the list of ids (patent ids and family ids)\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        self.patent_ids = df_main[Config.VAR_APPLN_ID].unique().tolist()\n",
    "        self.patent_family_ids = df_main[Config.VAR_DOCDC_FAMILY_ID].unique().tolist()\n",
    "        \n",
    "        # (2) Filter the tables according to the new list of patent ids\n",
    "        def __filter(df, var, list_ids):\n",
    "            \"\"\"Code snippet to filter a dataset according to a list of ids\"\"\"\n",
    "            condition = df[var].isin(list_ids)\n",
    "            return df[condition]\n",
    "        \n",
    "        for key in self.data:\n",
    "            if key in ['_table_main_patent_infos','_table_cpc','_table_patentees_info']:\n",
    "                self.data[key] = __filter(self.data[key], Config.VAR_APPLN_ID, self.patent_ids)\n",
    "            elif key in ['_table_backward_docdb_citations','_table_forward_docdb_citations']:\n",
    "                self.data[key] = __filter(self.data[key], Config.VAR_DOCDC_FAMILY_ID, self.patent_family_ids)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewMetrics:\n",
    "    \"\"\"Methods to derive new metrics from the data\"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _get_DOCDB_fam_cites_per_year(self):\n",
    "        \"\"\"Adding a variable to keep track of yearly citations by patent family\"\"\"\n",
    "        \n",
    "        # Unpacking some variables for clarity\n",
    "        df = self.data['_table_main_patent_infos']\n",
    "        citations_by_year = Config.NEW_VAR_NB_CITING_DOCDB_FAM_BY_YEAR\n",
    "        citations_docdb_fam = Config.VAR_NB_CITING_DOCDB_FAM\n",
    "        year = Config.VAR_APPLN_FILLING_YEAR\n",
    "        ref_year = Config.LAST_YEAR_TO_RECEIVE_CITAITONS\n",
    "        \n",
    "        # Compute the metric\n",
    "        df[citations_by_year] = df[citations_docdb_fam]/(ref_year-df[year])\n",
    "        \n",
    "        # Updating the table\n",
    "        self.TABLE_ALL_PATENTS_INFO = df \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a patent object. Since the patent will have a long list of attributes, \n",
    "# we stored their attributes in a dictionnary. As a shortcut, we store the main patent key \n",
    "# appln_id as an attribute direclty accesible with patent.appln_id.\n",
    "\n",
    "\n",
    "class Patent:\n",
    "    \n",
    "    def __init__(self, appln_id):\n",
    "        \"\"\"Setting the patent parameters\"\"\"\n",
    "        \n",
    "        self.appln_id:int # as a shortcut we  store the main patent key\n",
    "        self.patent_attributes = {} # Contains the list of the patent's attributes\n",
    "        \n",
    "        # Set instance attributes\n",
    "        self.patent_attributes.update({Config.VAR_APPLN_ID :  appln_id})\n",
    "        self.appln_id = appln_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping to OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a set of methods to reshape the data from the tabular form (as extracted from PATSTAT)\n",
    "# to an object oriented form, where patents are identified and attributes attributed to them.\n",
    "\n",
    "class ReshapingToOOP:\n",
    "    \"\"\"Methods to assign the data to patent objects\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _create_patent_objects(self):\n",
    "        \"\"\"\n",
    "        Create a Patent object for each patent id and store them in a list\n",
    "        \"\"\"\n",
    "        self.patent_list = []\n",
    "        for patent_id in list(self.patent_ids):\n",
    "            a = Patent(patent_id)\n",
    "            self.patent_list.append(a)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _assign_data_to_patent_obj(self):\n",
    "        \"\"\"\n",
    "        Once the data has been retrieved from PATSTAT and the patent objects\n",
    "        have been created, we assign the data to the Patent objects\n",
    "        \"\"\"\n",
    "        \n",
    "        def __snippet_store_patent_attributes(table):\n",
    "            \"\"\"\n",
    "            Code snippet to dynamically store attributes \n",
    "            from a Pandas table in a dictionnary\n",
    "            # If a value has several values, then ts stored in a list\n",
    "            \"\"\"\n",
    "            a = {}\n",
    "            for col in list(table):\n",
    "                key = col\n",
    "                value = table[col].unique().tolist()#[0]\n",
    "                value = [x for x in value if (x == x)!=False] # new line\n",
    "                if len(value) == 1:\n",
    "                    value = value[0]\n",
    "                a[key] = value\n",
    "            return a\n",
    "        \n",
    "        # Unpacking some variables\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        df_cpc = self.data['_table_cpc']\n",
    "        df_patentee = self.data['_table_patentees_info']\n",
    "        df_bwd = self.data['_table_backward_docdb_citations']\n",
    "        df_fwd = self.data['_table_forward_docdb_citations']\n",
    "        \n",
    "        # (1) Assigning the data contained in the main table to the patent\n",
    "        # We merge backward citation data to the main table (on family id)\n",
    "        key = Config.VAR_DOCDC_FAMILY_ID\n",
    "        df_main = pd.merge(df_main, df_bwd,how = 'left',left_on = key,right_on = key)\n",
    "        \n",
    "        for patent in self.patent_list:                \n",
    "            for df in [df_main, df_cpc, df_patentee]:  \n",
    "                patent_table = df[df[Config.VAR_APPLN_ID]==patent.appln_id]\n",
    "                d = __snippet_store_patent_attributes(table = patent_table)\n",
    "                patent.patent_attributes.update(d)\n",
    "        \n",
    "        # (2) Assigning forward citations to the patents      \n",
    "        df_fwd.columns = ['A','B','C'] # Random column names\n",
    "        for patent in self.patent_list:\n",
    "            patent_fam_table = df_fwd[df_fwd['A']==patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID]]\n",
    "            citing_fam = patent_fam_table['B'].unique().tolist()\n",
    "            patent.patent_attributes.update({Config.NEW_VAR_CITING_DOCDB_FAM_IDS :citing_fam})\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get citations\n",
    "We use the similarity measure to link the patents in the network. We use direct and indirect citation links: 􏰀\n",
    "* Direct backwards citation (at the patent family level); 􏰀\n",
    "* Co-citations (CC);\n",
    "* Biographic coupling (BC);\n",
    "* Longitudinal coupling (LC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCitations:\n",
    "    \"\"\"Methods to compute direct and indirect (BC, CC, LC) citations between the patents\"\"\"\n",
    "        \n",
    "    def _get_direct_citations(self):\n",
    "        \"\"\"Get direct backwards citations (at the level of the family level)\"\"\"\n",
    "        \n",
    "        # Unpacking some varibles for clarity\n",
    "        fam = Config.VAR_DOCDC_FAMILY_ID\n",
    "        cited_fam = Config.VAR_CITED_DOCDB_FAM_ID\n",
    "            \n",
    "        # (1) If a patent cites only one family\n",
    "        list1 = [(x,y) for x in self.patent_list for y in self.patent_list \\\n",
    "                 if y.patent_attributes[fam] == x.patent_attributes[cited_fam]]\n",
    "        \n",
    "        # (2) If the patent cites several families (then stored as list)\n",
    "        list2 = [(x,y) for x in self.patent_list for y in self.patent_list \\\n",
    "                 if type(x.patent_attributes[cited_fam]) ==list \\\n",
    "                 if y.patent_attributes[fam] in x.patent_attributes[cited_fam]]\n",
    "        \n",
    "        # Concatenating the two lists to have the direct citations\n",
    "        self.direct_citations = list1 + list2\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _get_BC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) Bibliographic coupling occurs when two works reference a common third work\n",
    "        # (2) The produced list is non directed.\n",
    "        # (3) Can be optimised\n",
    "        \"\"\"\n",
    "            \n",
    "        # Definition of variables\n",
    "        BC = []\n",
    "        a = self.patent_list\n",
    "        all_patent_pairs = [(a[p1], a[p2]) for p1 in range(len(a)) for p2 in range(p1+1,len(a))]\n",
    "\n",
    "        # Computing BC by looping over all pairs of patents\n",
    "        for patent_1, patent_2 in all_patent_pairs:\n",
    "            list_citing_1 = patent_1.patent_attributes[Config.NEW_VAR_CITING_DOCDB_FAM_IDS]\n",
    "            list_citing_2 = patent_2.patent_attributes[Config.NEW_VAR_CITING_DOCDB_FAM_IDS]\n",
    "            common_elements = [x for x in list_citing_1 if x in list_citing_2]\n",
    "            if len(common_elements)>0:\n",
    "                BC.append((patent_1, patent_2))\n",
    "            \n",
    "        # Removing duplicated items in the list\n",
    "        self.BC = list(set(BC)) \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _get_CC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) Co-citation is defined as the frequency with which two documents are cited together\n",
    "        by other documents. If at least one other document cites two documents in common these documents\n",
    "        are said to be co-cited\n",
    "        # (2) The produced list is non directed\n",
    "        \"\"\"\n",
    "        CC = []\n",
    "            \n",
    "        # Definition of all patent pairs\n",
    "        a = self.patent_list\n",
    "        all_patent_pairs = [(a[p1], a[p2]) for p1 in range(len(a)) for p2 in range(p1+1,len(a))]\n",
    "            \n",
    "        # Definition of the search algorithm\n",
    "        for patent in self.patent_list:\n",
    "            a = patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "            if type(a)==list:\n",
    "                if len(a)>1:\n",
    "                    all_cited_patent_pairs = [(a[p1], a[p2]) \\\n",
    "                                              for p1 in range(len(a)) \\\n",
    "                                              for p2 in range(p1+1,len(a))]\n",
    "                    for pair in all_cited_patent_pairs:\n",
    "                        CC.append(pair)\n",
    "        \n",
    "        pairs = list(set(CC)) \n",
    "        \n",
    "        CC = []\n",
    "        for pair in pairs:\n",
    "            patent1 = [patent \\\n",
    "                       for patent in self.patent_list \\\n",
    "                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] == pair[0]]\n",
    "            patent2 = [patent \\\n",
    "                       for patent in self.patent_list \\\n",
    "                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] == pair[1]]\n",
    "\n",
    "            if len(patent1)>0:\n",
    "                patent1 = patent1[0]\n",
    "            else: patent1=np.nan\n",
    "\n",
    "            if len(patent2)>0:\n",
    "                patent2 = patent2[0]\n",
    "            else: patent2=np.nan\n",
    "\n",
    "            pair = (patent1, patent2)\n",
    "            CC.append(pair)\n",
    "\n",
    "        self.CC = [pair for pair in CC if (pair[0]==pair[0]) & (pair[1] == pair[1])]\n",
    "        return self\n",
    "     \n",
    "        \n",
    "    def _get_LC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) LC (longitudinal coupling). A cites a document that cites B\n",
    "        # (2) The produced list IS directed \n",
    "        # (3) Can be optimised\n",
    "        \"\"\"          \n",
    "        LC = []\n",
    "            \n",
    "        # Identifying all patents cited by a given patent A\n",
    "        for patent_A in self.patent_list:\n",
    "            cited_fam = patent_A.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "            if type(cited_fam)==float:\n",
    "                    cited_fam = []\n",
    "                    cited_fam.append(patent_A.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID])\n",
    "            cited_patents = [patent \\\n",
    "                             for patent in self.patent_list \\\n",
    "                             if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] in cited_fam]\n",
    "                \n",
    "            # Identifying all patents cited by a patent cited by patent A\n",
    "            for cited_patent in cited_patents:\n",
    "                cited_fam = cited_patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "                if type(cited_fam)==float:\n",
    "                    cited_fam = []\n",
    "                    cited_fam.append(cited_patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID])\n",
    "                cited_cited_patents = [patent \\\n",
    "                                       for patent in self.patent_list \\\n",
    "                                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] in cited_fam]\n",
    "                    \n",
    "                # Adding the pairs in the LC list\n",
    "                for patent_B in cited_cited_patents:\n",
    "                    LC.append((patent_A, patent_B))\n",
    "                    \n",
    "        # Removing duplicated items in the LC list\n",
    "        self.LC = list(set(LC)) \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriving the full text data for the patents of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveFullTextData:\n",
    "    \"\"\"\n",
    "    Methods to retrieve the full text data to the selected patents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _assign_full_text_to_patents(self):\n",
    "        \"\"\"\n",
    "        For each patent contained in the model, assigns in the patent attributes\n",
    "        under the 'full_text' entry a dataframe containing all the text of this \n",
    "        patent, in raw format\n",
    "        \"\"\"\n",
    "        \n",
    "        lista = [str(x) for x in list(self.data['_text_data']['publication_number'])]\n",
    "        df = self.data['_text_data']\n",
    "        cols = list(df)\n",
    "\n",
    "        for patent in self.patent_list:\n",
    "            if patent.patent_attributes['publn_nr'] in lista:\n",
    "                data = df[df['publication_number'] == int(patent.patent_attributes['publn_nr'])]\n",
    "                patent.patent_attributes['full_text'] = data\n",
    "            else:\n",
    "                patent.patent_attributes['full_text'] = pd.DataFrame(columns=cols)\n",
    "        return self\n",
    "           \n",
    "        \n",
    "    @staticmethod    \n",
    "    def _translate(text_to_translate, source_language = False, target_language = 'en'):\n",
    "        \"\"\"Machine translation using the Google translate API\"\"\"\n",
    "\n",
    "        # initialise the translator object (see the googletrans API docs)\n",
    "        translator = Translator()\n",
    "        # behaviour of the function if the input language is known\n",
    "        if source_language != \"False\":\n",
    "            translatedText = translator.translate(text_to_translate, src=source_language, dest=target_language)\n",
    "        # if the language of the input is not known\n",
    "        else: \n",
    "            translatedText = translator.translate(text_to_translate, dest=target_language)\n",
    "        return translatedText.text\n",
    "\n",
    "    \n",
    "    @staticmethod \n",
    "    def _get_text_claims(data_patent_text):\n",
    "        \"\"\"For a given dataframe coming from the EP full-text database, returns\n",
    "        a list of its claim\"\"\"\n",
    "\n",
    "        ## data manipulation\n",
    "\n",
    "        # nickname \n",
    "        data = data_patent_text\n",
    "        # select only the claims in the dataframe\n",
    "        data = data[data['text_type']=='CLAIM']\n",
    "        # sort by data\n",
    "        data = data.sort_values(by = 'publication_date', ascending = False)\n",
    "        # keep only most recent claims by language\n",
    "        data.drop_duplicates(subset = ['language_text_component','text_type'], inplace = True)\n",
    "        # keep languages according to the other EN, DE, FR, other ('xx') (for best consistency of the translating)\n",
    "        data['language_text_component'] = pd.Categorical(data['language_text_component'] , categories=[\"en\",\"de\",\"fr\",\"xx\"], ordered=True)\n",
    "        data = data.sort_values(by = 'language_text_component')\n",
    "        data.drop_duplicates(subset = ['text_type'], inplace = True)\n",
    "\n",
    "        # if the data contains no claims then the data selected is empty\n",
    "        if len(data)==0:\n",
    "            return ['Unavailable']\n",
    "\n",
    "        else:\n",
    "\n",
    "            # store the language at this point for translation at the end\n",
    "            language = data.iloc[0]['language_text_component']\n",
    "            # selection of the field of the pandas dataframe which contains the claims texts\n",
    "            text_xml = data.iloc[0]['text']\n",
    "\n",
    "            ## Process the xml to get the raw text\n",
    "\n",
    "            # modifying the claim to be processed as a real xml\n",
    "            text_xml_modified = \"<data>\" + text_xml + '</data>'\n",
    "            # we parse it with the ElementTree XML API¶\n",
    "            root = ET.fromstring(text_xml_modified)\n",
    "            # and this is how we access the text of the claims\n",
    "            claims = root.findall(\"./claim/claim-text\")\n",
    "            # we store the claims in a list\n",
    "            claims_text = [claim.text for claim in claims]\n",
    "\n",
    "            ## Translate if the claims are not in EN\n",
    "            if language =='en':\n",
    "                pass\n",
    "            elif language == 'de':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, 'de', 'en') for text in claims_text]\n",
    "            elif language == 'fr':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, 'fr', 'en') for text in claims_text]\n",
    "            elif language == 'xx':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, False, 'en') for text in claims_text]\n",
    "            else:\n",
    "                claims_text = [RetrieveFullTextData.translate(text, False, 'en') for text in claims_text]\n",
    "\n",
    "            return claims_text\n",
    "\n",
    "\n",
    "    def _attribute_claims(self):\n",
    "        \"\"\"For each patent, uses the _get_text_claims function to store the claims text in the object\"\"\"\n",
    "        for patent in self.patent_list:\n",
    "            patent.patent_attributes['full_text_claims'] = RetrieveFullTextData._get_text_claims(patent.patent_attributes['full_text'])\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing (work in progress)\n",
    "\n",
    "- stemming\n",
    "- vectorisation with TF-IDF\n",
    "- measure cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStemmer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"We overwrite the Sklearn BaseEstimator class in order to have more control on the \n",
    "    text data preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, stemmer_type):\n",
    "        \"\"\"We can use different types of stemmer\"\"\"\n",
    "        \n",
    "        self.tokenizer = RegexpTokenizer(\"(?u)\\\\b[\\\\w-]+\\\\b\")\n",
    "        self.stemmer_type = stemmer_type\n",
    "        \n",
    "        if stemmer_type == 'snowball':\n",
    "            self.stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        \"\"\"Overwritten for the sake of completeness, does not perform any action\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Returns a stemmed version of the documents, using the Porter algorithm (snowball)\n",
    "        and removing English stop words\"\"\"\n",
    "        \n",
    "        if self.stemmer_type!='no':\n",
    "            documents = documents.apply(self.tokenizer.tokenize)\n",
    "\n",
    "            def snowball(x):\n",
    "                liste = [self.stemmer.stem(y) for y in x]\n",
    "                return liste\n",
    "\n",
    "            documents = documents.apply(snowball)\n",
    "            documents = [' '.join(docs) for docs in documents]\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"We overwrite the Sklearn BaseEstimator class in order to have more control on the vectorisation\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_type):\n",
    "        \"\"\"Two possibilities here: count and tfidf. More can be added if neccessary\"\"\"\n",
    "        \n",
    "        if vectorizer_type == 'count':\n",
    "            self.vectorizer = CountVectorizer(binary=True)\n",
    "    \n",
    "        if vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        \"\"\"Does not perform any action\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Return a numpy arraw - the feature space\"\"\"\n",
    "        freqs = self.vectorizer.fit_transform(documents)\n",
    "        return [freq.toarray()[0] for freq in freqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessing:\n",
    "    \"\"\"\n",
    "    Methods for text analysis and similarity measures, using the 2 classes above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _index_patents(self):\n",
    "        \"\"\"We create an index of patents, in order to be able to process all the text together in\n",
    "        the following text processing steps and still be able to access individual patent text data\"\"\"\n",
    "        \n",
    "        self.dict_patents_indexes = {k: v for v, k in enumerate(self.patent_list)}\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _store_vocabulary(self):\n",
    "        \"\"\"Store all the vocabulary contained in the patents in a Panda series called 'corpus' \"\"\"\n",
    "        \n",
    "        # create an empty list and fill it with the claim text of each patent in a separate entry\n",
    "        l = []\n",
    "        for patent in self.patent_list:\n",
    "            l.append(patent.patent_attributes['full_text_claims'])\n",
    "        # flatten the nested list by joining the sentences/claims together in a single sentence\n",
    "        # for each patent / except if the text is unavailable\n",
    "        l = [' '.join(element) for element in l if isinstance(element, str)==False]\n",
    "        # reshape as a single Pandas serie and store in the corpus\n",
    "        self.corpus = pd.DataFrame(l, columns=['text']).pop('text')\n",
    "        return self\n",
    "    \n",
    "    def _stemming(self):\n",
    "        \"\"\"Reducing words to their stem word (semantic root), and remove the English stop words\"\"\"\n",
    "        \n",
    "        stemmer = CustomStemmer('snowball')\n",
    "        self.corpus_stemmed = stemmer.transform(self.corpus)\n",
    "        \n",
    "        ### HERE FOR ALL THE PATENTS STORE THE STEMMED TEXT IN THE ATTRIBUTES\n",
    "        return self\n",
    "    \n",
    "    def _vectorize(self):\n",
    "        \"\"\"Vectorise the patents in a high dimention space (returns a list)\"\"\"\n",
    "        \n",
    "        custom_vectorizer = CustomVectorizer('tfidf')\n",
    "        self.feature_space = custom_vectorizer.fit_transform(self.corpus_stemmed)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def _compute_pairwise_similarities(self):\n",
    "        \"\"\"returns a numpy.ndarray containing all pairwise similarities between patents\"\"\"\n",
    "        \n",
    "        # https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
    "        # To compute the cosine distance of the first doc to all the others \n",
    "        from sklearn.metrics.pairwise import linear_kernel\n",
    "        # in this case linear_kernel is equivalent to cosine_similarity because the TfidfVectorizer produces normalized vectors.\n",
    "        # returs an array with all pairwise similarities!\n",
    "        self.cosine_similarities = linear_kernel(self.feature_space, self.feature_space)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _similarity(patent1, patent2):\n",
    "        \"\"\"Measure the similiarity between a pair of linked patents pair = (patent1, patent2)\"\"\"\n",
    "        i = self.dict_patents_indexes[patent1]\n",
    "        j = self.dict_patents_indexes[patent2]\n",
    "        return self.cosine_similarities[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the nlp-based patent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildNetwork():\n",
    "    \"\"\"Builds a weighted network based on backwards citations and text similarity\"\"\"\n",
    "    \n",
    "    def _create_network(self):\n",
    "        \"\"\"Create the weighted and undirected network with igraph\"\"\"\n",
    "        \n",
    "        # defining all possible links between any pair of patents\n",
    "        links = self.direct_citations + self.CC + self.BC + self.LC\n",
    "        \n",
    "        def filter_symmetric_duplicates(l:list):\n",
    "            \"\"\"Code snippet to filter symmetric duplicates in a list of tuples\n",
    "            Eg [(1,2), (2,1)] -> [(1,2)]\"\"\"\n",
    "            seen = []\n",
    "            for pair in l:\n",
    "                if pair in seen:\n",
    "                    l.remove(pair)\n",
    "                seen.append(tuple(reversed(pair)))\n",
    "            return l\n",
    "        \n",
    "        # definition of the links\n",
    "        links = filter_symmetric_duplicates(links)\n",
    "        weighted_links = [(p1, p2, TextProcessing._similarity(p1, p2)) for (p1, p2) in links]\n",
    "        # creation of the graph\n",
    "        self.graph = Graph.TupleList(weighted_links, weights=True)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryStatistics:\n",
    "    \"\"\"Summary statistics for the data section\"\"\"\n",
    "    # Can also help comparing before data cleaning and after!\n",
    "    \n",
    "    def _print_nb_patents(self):\n",
    "        \"\"\"Printing info\"\"\"\n",
    "        print('..Nb of patents:',len(self.data['_table_main_patent_infos']\\\n",
    "                                     [Config.VAR_APPLN_ID].unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualisation:\n",
    "    \"\"\"Visualisation methods\"\"\"\n",
    "    \n",
    "    def _draw_graph_with_communities(self):\n",
    "        comms = model.graph.community_multilevel()\n",
    "        display(igraph.plot(comms, mark_groups = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Config, DataCleaning, NewMetrics, ReshapingToOOP, GetCitations, RetrieveFullTextData,\n",
    "            TextProcessing, BuildNetwork, SummaryStatistics, Visualisation):\n",
    "    \"\"\"\n",
    "    Creation of a model which inherits several building blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attributes of the model\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data: dict # datasets\n",
    "        patent_list: list # patent objects\n",
    "        dict_patents_indexes: dict # mapping of patents objects and their indexes\n",
    "        patent_ids: list # list of patent ids contained in the model\n",
    "        patent_family_ids: list # list of DOCDB family ids contained in the model\n",
    "        direct_citations: list # directed list of simple citations\n",
    "        CC: list # undirected list of co-citations\n",
    "        BC: list # undirected list of bibliographical coupling\n",
    "        LC: list # directed list of longitudinal citations\n",
    "        corpus: pandas.core.series.Series # contains all claim text of each patent (raw)\n",
    "        corpus_stemmed: pandas.core.series.Series # contains all claim text of each patent (stemmed)\n",
    "        feature_space: list # the feature space, the high dimension representation of the text data\n",
    "        cosine_similarities: numpy.ndarray # contains all the pairwise similarities between patents\n",
    "        graph: igraph.Graph # Igraph network \n",
    "        \n",
    "    \n",
    "    def _input_data(self, data):\n",
    "        \"\"\"Getting the data in the model\"\"\"\n",
    "        self.data = data\n",
    "    \n",
    "    \n",
    "    def _compute_new_metrics(self):\n",
    "        \"\"\"Adding new variables in the dataset\"\"\"\n",
    "        self = NewMetrics._get_DOCDB_fam_cites_per_year(self)  \n",
    "    \n",
    "    \n",
    "    def _data_cleaning(self):\n",
    "        \"\"\"Data cleaning using the DataCleaning class methods\"\"\"\n",
    "        self = DataCleaning._correct_JP_data(self)\n",
    "        self = DataCleaning._normalise(self)\n",
    "        self = DataCleaning._select_one_patent_per_family(self)\n",
    "        self = DataCleaning._select_breakthrough_patents(self)\n",
    "    \n",
    "    \n",
    "    def _fit_to_object_oriented_design(self):\n",
    "        \"\"\"We reshape the data from a tabular form to an object oriented form\"\"\"\n",
    "        self = ReshapingToOOP._create_patent_objects(self)\n",
    "        self = ReshapingToOOP._assign_data_to_patent_obj(self)  \n",
    "   \n",
    "\n",
    "    def _get_citations(self):\n",
    "        \"\"\"Identify direct and indirect citations that link the patents\"\"\"\n",
    "        self = GetCitations._get_direct_citations(self)\n",
    "        self = GetCitations._get_CC_citations(self)\n",
    "        self = GetCitations._get_BC_citations(self)\n",
    "        self = GetCitations._get_LC_citations(self)\n",
    "        \n",
    "    \n",
    "    def _get_full_text(self):\n",
    "        \"\"\"Retrieve the full text data corresponding to the patents in the model, \n",
    "        extract the claims and attribute them to the patent objects\"\"\"\n",
    "        self = RetrieveFullTextData._assign_full_text_to_patents(self)\n",
    "        self = RetrieveFullTextData._attribute_claims(self)\n",
    "    \n",
    "    \n",
    "    def _text_preprocessing(self):\n",
    "        \"\"\"Computing text similarities between linked patents\n",
    "        \n",
    "        # 1. index all the patents in the 'dict_patents_indexes' dictionnary\n",
    "        # 2. get all the vocabulary stored in a single Pandas serie\n",
    "        # 3. text preprocessing\n",
    "        # 4. vectorisation and create the feature space \n",
    "        # 5. compute the array of all pairwise similarities from the feature space\n",
    "        \"\"\"\n",
    "        self = TextProcessing._index_patents(self)\n",
    "        self = TextProcessing._store_vocabulary(self)\n",
    "        self = TextProcessing._stemming(self)\n",
    "        self = TextProcessing._vectorize(self)\n",
    "        self = TextProcessing._compute_pairwise_similarities(self)\n",
    "     \n",
    "        \n",
    "    def _build_patent_network(self):\n",
    "        \"\"\"We build the patent network (weighted directed graph)\"\"\"\n",
    "        self = BuildNetwork._create_network(self)\n",
    "    \n",
    "    \n",
    "    def _visualise(self):\n",
    "        \"\"\"Plot\"\"\"\n",
    "        self = Visualisation._draw_graph_with_communities(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation of the model\n",
    "model = Model()\n",
    "# fitting the model to the data\n",
    "model._input_data(data)\n",
    "# new metrics\n",
    "model._compute_new_metrics()\n",
    "# data cleaning\n",
    "model._data_cleaning()\n",
    "# reshape in an OOP manner before building the network\n",
    "model._fit_to_object_oriented_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving all types of citations\n",
    "model._get_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in the full text database the patents of interest\n",
    "model._get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing, construction of the feature space and computation of all pairwise similarities\n",
    "model._text_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builing the NLP-based patent network\n",
    "model._build_patent_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot a visualisation\n",
    "model._visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._print_nb_patents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of the contains of the patent full_text_claims\n",
    "for patent in model.patent_list:\n",
    "    print(patent.patent_attributes['full_text_claims'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1. get all the vocabulary stored in a single Pandas serie\n",
    "# 2. text preprocessing\n",
    "# 3. vectorisation and create the feature space \n",
    "# 4. compute the array of all pairwise similarities from the feature space\n",
    "# 4. return any pairwise cosine similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def store_patent_vocabulary():\n",
    "    \"\"\"Get all the vocabulary from all patents stored in a single Pandas serie\"\"\"\n",
    "    \n",
    "    # create an empty list and fill it with the claim text of each patent in a separate entry\n",
    "    l = []\n",
    "    for patent in model.patent_list:\n",
    "        l.append(patent.patent_attributes['full_text_claims'])\n",
    "    # flatten the nested list by joining the sentences/claims together in a single sentence\n",
    "    # for each patent / except if the text is unavailable\n",
    "    l = [' '.join(element) for element in l if isinstance(element, str)==False]\n",
    "    # reshape as a single Pandas serie\n",
    "    corpus = pd.DataFrame(l, columns=['text']).pop('text')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = store_patent_vocabulary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stemmer = CustomStemmer('snowball')\n",
    "corpus_stemmed = stemmer.transform(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "#tfidf = TfidfVectorizer()\n",
    "#feature_space = tfidf.fit_transform(corpus_stemmed)\n",
    "custom_vectorizer = CustomVectorizer('tfidf')\n",
    "feature_space = custom_vectorizer.fit_transform(corpus_stemmed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
    "# To compute the cosine distance of the first doc to all the others \n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# in this case linear_kernel is equivalent to cosine_similarity because the TfidfVectorizer produces normalized vectors.\n",
    "# returs an array with all pairwise similarities!\n",
    "cosine_similarities = linear_kernel(feature_space, feature_space)\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import itertools\n",
    "\n",
    "# assign an index to each patent\n",
    "dict_patents_indexes = {k: v for v, k in enumerate(model.patent_list)}\n",
    "\n",
    "# iterating through all pairs of patents to get their pairwise similarity\n",
    "for patent1, patent2 in itertools.product(model.patent_list, repeat=2):\n",
    "    i = dict_patents_indexes[patent1]\n",
    "    j = dict_patents_indexes[patent2]\n",
    "    print(cosine_similarities[i,j])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(feature_space)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_space"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through all pairs of patents to get their pairwise similarity\n",
    "for patent in model.patent_list:\n",
    "    print(dict_patents_indexes[patent])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
