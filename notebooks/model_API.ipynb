{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of emerging technologies using NLP-powered patent networks\n",
    "\n",
    "## Model API\n",
    "\n",
    "* **Contains the model and its instances in order to be called from other modules**, for instance for robustness checks or applications. \n",
    "* More details about the implementation choices and the rationale behind the algorithms can be found in the `main model` notebook.\n",
    "\n",
    "> Author: **Antoine MATHIEU COLLIN**\n",
    "* Department of Management, Strategy and Innovation (MSI) of the Faculty of Economics and Business (FEB), KU Leuven\n",
    "* Department of Computer Science of the Faculty of Engineering Science, KU Leuven\n",
    "* Leuven.AI, KU Leuven Institute for Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import igraph\n",
    "from igraph import Graph\n",
    "import random\n",
    "\n",
    "# language processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "# machine learning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud \n",
    "from igraph import BoundingBox, palettes\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# library to parse the xml content of the EP full text database\n",
    "# library doc: https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "import xml.etree.ElementTree as ET  \n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\" \n",
    "    # Configuration\n",
    "    The config class can be overwritten when calling the model from outside this notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    # Magic numbers\n",
    "    LAST_YEAR_TO_RECEIVE_CITAITONS = 2018\n",
    "    FIRST_YEAR_CONVERAGE_EP_FULL_TEXT = 2000\n",
    "    LAST_YEAR_COVERAGE_PATSTAT = 2014\n",
    "    GRANTED_PATENT_CODE = 1\n",
    "    PERCENTAGE_TOP_PATENTS = 0.2\n",
    "    EP_AUTHORITY = 'EP'\n",
    "\n",
    "    # PASTAT_variables \n",
    "    VAR_APPLN_ID = 'appln_id'\n",
    "    VAR_DOCDC_FAMILY_ID = 'docdb_family_id'\n",
    "    VAR_CITED_DOCDB_FAM_ID = 'cited_docdb_family_id'\n",
    "    VAR_APPLN_FILLING_YEAR = 'appln_filing_year'\n",
    "    VAR_NB_CITING_DOCDB_FAM = 'nb_citing_docdb_fam'\n",
    "    VAR_EARLIEST_FILLING_DATE = 'earliest_filing_date'\n",
    "    VAR_EARLIEST_FILING_YEAR = 'earliest_filing_year'\n",
    "\n",
    "    # Computed variables\n",
    "    NEW_VAR_CITING_DOCDB_FAM_IDS = 'citing_docdb_families_ids'\n",
    "    NEW_VAR_NB_CITING_DOCDB_FAM_BY_YEAR = 'nb_citing_docdb_fam_by_year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     10,
     24,
     38,
     54,
     61,
     85,
     111
    ]
   },
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    # Data cleaning methods.\n",
    "    The data should be inputed in a specific format detailed in the 'main_model' notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _keep_only_EP_patents(self):\n",
    "        \"\"\"We filter the data to keep only EU patents (not only EP)\"\"\"\n",
    "        \n",
    "        # Local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        # filtering\n",
    "        condition = df_main['appln_auth']==Config.EP_AUTHORITY\n",
    "        df_main = df_main[condition]\n",
    "        # update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = df_main\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _keep_only_granted_patents(self):\n",
    "        \"\"\"We keep only patents which have gone through the approval process, ie are granted\"\"\"\n",
    "        \n",
    "        # local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        # filtering\n",
    "        condition = df_main['granted']==Config.GRANTED_PATENT_CODE\n",
    "        df_main = df_main[condition]\n",
    "        # update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = df_main\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _select_time_range(self):\n",
    "        \"\"\"We do not keep patents before 2000, because no full text is available\"\"\"\n",
    "        \n",
    "        # local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        # filtering\n",
    "        condition1 = df_main['appln_filing_year']>=Config.FIRST_YEAR_CONVERAGE_EP_FULL_TEXT\n",
    "        condition2 = df_main['appln_filing_year']<=Config.LAST_YEAR_COVERAGE_PATSTAT\n",
    "        df_main = df_main[condition1 & condition2]\n",
    "        # update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = df_main\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # this function is not implemented\n",
    "    def _normalise(self):\n",
    "        \"\"\"Normalisation of the data accross years and sectors, to cater for **patent explosion**\"\"\"\n",
    "        # Do # Update the list of ids\n",
    "        self = self.__update_patent_fam_ids() # Storing ids and filtering datasets\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _select_one_patent_per_family(self):\n",
    "        \"\"\"In order to select only patent of interest, as well as\n",
    "        saving computationnal power, we select only the earliest patent by\n",
    "        family\"\"\"\n",
    "        \n",
    "        # Local variables for simplicity\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        df_cpc = self.data['_table_cpc']\n",
    "        df_patentees = self.data['_table_patentees_info']\n",
    "        \n",
    "        # Filtering \n",
    "        df_main.sort_values(by = Config.VAR_EARLIEST_FILLING_DATE,inplace = True)\n",
    "        df_main.drop_duplicates(subset = [Config.VAR_DOCDC_FAMILY_ID],\n",
    "                                keep = 'first',\n",
    "                                inplace = True)\n",
    "        \n",
    "        # Update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = df_main\n",
    "        \n",
    "        # Storing ids and filtering datasets\n",
    "        self = self.__update_patent_fam_ids()   \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def _select_breakthrough_patents(self):\n",
    "        \"\"\"Filtering the data to keep only breakthrough patents\"\"\"\n",
    "        \n",
    "        # Unpacking some variables for clarity\n",
    "        X = Config.PERCENTAGE_TOP_PATENTS\n",
    "        df = self.data['_table_main_patent_infos']\n",
    "        \n",
    "        # Selection  of the top patents\n",
    "        filtered_df = pd.DataFrame()\n",
    "        for year in df[Config.VAR_EARLIEST_FILING_YEAR].unique().tolist():\n",
    "            df_year = df[df[Config.VAR_EARLIEST_FILING_YEAR] == year]\n",
    "            df_year.sort_values(by = Config.VAR_NB_CITING_DOCDB_FAM,\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "            nb_top_patent_given_year = int(math.ceil(X*len(df_year))) # Needs rounding up\n",
    "            df_year = df_year.head(nb_top_patent_given_year)\n",
    "            filtered_df = pd.concat([filtered_df, df_year])\n",
    "            \n",
    "        # Update the table and the list of patent/fam ids\n",
    "        self.data['_table_main_patent_infos'] = filtered_df\n",
    "        \n",
    "        # Storing ids and filtering datasets\n",
    "        self = self.__update_patent_fam_ids()\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __update_patent_fam_ids(self):\n",
    "        \"\"\"\n",
    "        # code snippet\n",
    "        \n",
    "        Storing patents ids and family ids and filtering the datasets\n",
    "        # Filtering the first 3 datasets on the list of patent ids \n",
    "        # Filtering the other 2 datasets on the list of family ids\n",
    "        \"\"\"\n",
    "        \n",
    "        # (1) Update the list of ids (patent ids and family ids)\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        self.patent_ids = df_main[Config.VAR_APPLN_ID].unique().tolist()\n",
    "        self.patent_family_ids = df_main[Config.VAR_DOCDC_FAMILY_ID].unique().tolist()\n",
    "        \n",
    "        # (2) Filter the tables according to the new list of patent ids\n",
    "        def __filter(df, var, list_ids):\n",
    "            \"\"\"Code snippet to filter a dataset according to a list of ids\"\"\"\n",
    "            condition = df[var].isin(list_ids)\n",
    "            return df[condition]\n",
    "        \n",
    "        for key in self.data:\n",
    "            if key in ['_table_main_patent_infos','_table_cpc','_table_patentees_info']:\n",
    "                self.data[key] = __filter(self.data[key], Config.VAR_APPLN_ID, self.patent_ids)\n",
    "            elif key in ['_table_backward_docdb_citations','_table_forward_docdb_citations']:\n",
    "                self.data[key] = __filter(self.data[key], Config.VAR_DOCDC_FAMILY_ID, self.patent_family_ids)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NewMetrics:\n",
    "    \"\"\"Methods to derive new metrics from the data\"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _get_DOCDB_fam_cites_per_year(self):\n",
    "        \"\"\"Adding a variable to keep track of yearly citations by patent family\"\"\"\n",
    "        \n",
    "        # Unpacking some variables for clarity\n",
    "        df = self.data['_table_main_patent_infos']\n",
    "        citations_by_year = Config.NEW_VAR_NB_CITING_DOCDB_FAM_BY_YEAR\n",
    "        citations_docdb_fam = Config.VAR_NB_CITING_DOCDB_FAM\n",
    "        year = Config.VAR_APPLN_FILLING_YEAR\n",
    "        ref_year = Config.LAST_YEAR_TO_RECEIVE_CITAITONS\n",
    "        \n",
    "        # Compute the metric\n",
    "        df[citations_by_year] = df[citations_docdb_fam]/(ref_year-df[year])\n",
    "        \n",
    "        # Updating the table\n",
    "        self.TABLE_ALL_PATENTS_INFO = df \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Patent:\n",
    "    \n",
    "    def __init__(self, appln_id):\n",
    "        \"\"\"Setting the patent parameters\"\"\"\n",
    "        \n",
    "        self.appln_id:int # as a shortcut we  store the main patent key\n",
    "        self.patent_attributes = {} # Contains the list of the patent's attributes\n",
    "        \n",
    "        # Set instance attributes\n",
    "        self.patent_attributes.update({Config.VAR_APPLN_ID :  appln_id})\n",
    "        self.appln_id = appln_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ReshapingToOOP:\n",
    "    \"\"\"Methods to assign the data to patent objects\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _create_patent_objects(self):\n",
    "        \"\"\"\n",
    "        Create a Patent object for each patent id and store them in a list\n",
    "        \"\"\"\n",
    "        self.patent_list = []\n",
    "        for patent_id in list(self.patent_ids):\n",
    "            a = Patent(patent_id)\n",
    "            self.patent_list.append(a)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _assign_data_to_patent_obj(self):\n",
    "        \"\"\"\n",
    "        Once the data has been retrieved from PATSTAT and the patent objects\n",
    "        have been created, we assign the data to the Patent objects\n",
    "        \"\"\"\n",
    "        \n",
    "        def __snippet_store_patent_attributes(table):\n",
    "            \"\"\"\n",
    "            Code snippet to dynamically store attributes \n",
    "            from a Pandas table in a dictionnary\n",
    "            # If a value has several values, then ts stored in a list\n",
    "            \"\"\"\n",
    "            a = {}\n",
    "            for col in list(table):\n",
    "                key = col\n",
    "                value = table[col].unique().tolist()#[0]\n",
    "                value = [x for x in value if (x == x)!=False] # new line\n",
    "                if len(value) == 1:\n",
    "                    value = value[0]\n",
    "                a[key] = value\n",
    "            return a\n",
    "        \n",
    "        # Unpacking some variables\n",
    "        df_main = self.data['_table_main_patent_infos']\n",
    "        df_cpc = self.data['_table_cpc']\n",
    "        df_patentee = self.data['_table_patentees_info']\n",
    "        df_bwd = self.data['_table_backward_docdb_citations']\n",
    "        df_fwd = self.data['_table_forward_docdb_citations']\n",
    "        \n",
    "        # (1) Assigning the data contained in the main table to the patent\n",
    "        # We merge backward citation data to the main table (on family id)\n",
    "        key = Config.VAR_DOCDC_FAMILY_ID\n",
    "        df_main = pd.merge(df_main, df_bwd,how = 'left',left_on = key,right_on = key)\n",
    "        \n",
    "        for patent in self.patent_list:                \n",
    "            for df in [df_main, df_cpc, df_patentee]:  \n",
    "                patent_table = df[df[Config.VAR_APPLN_ID]==patent.appln_id]\n",
    "                d = __snippet_store_patent_attributes(table = patent_table)\n",
    "                patent.patent_attributes.update(d)\n",
    "        \n",
    "        # (2) Assigning forward citations to the patents      \n",
    "        df_fwd.columns = ['A','B','C'] # Random column names\n",
    "        for patent in self.patent_list:\n",
    "            patent_fam_table = df_fwd[df_fwd['A']==patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID]]\n",
    "            citing_fam = patent_fam_table['B'].unique().tolist()\n",
    "            patent.patent_attributes.update({Config.NEW_VAR_CITING_DOCDB_FAM_IDS :citing_fam})\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     24,
     49,
     99
    ]
   },
   "outputs": [],
   "source": [
    "class GetCitations:\n",
    "    \"\"\"Methods to compute direct and indirect (BC, CC, LC) citations between the patents\"\"\"\n",
    "        \n",
    "    def _get_direct_citations(self):\n",
    "        \"\"\"Get direct backwards citations (at the level of the family level)\"\"\"\n",
    "        \n",
    "        # Unpacking some varibles for clarity\n",
    "        fam = Config.VAR_DOCDC_FAMILY_ID\n",
    "        cited_fam = Config.VAR_CITED_DOCDB_FAM_ID\n",
    "            \n",
    "        # (1) If a patent cites only one family\n",
    "        list1 = [(x,y) for x in self.patent_list for y in self.patent_list \\\n",
    "                 if y.patent_attributes[fam] == x.patent_attributes[cited_fam]]\n",
    "        \n",
    "        # (2) If the patent cites several families (then stored as list)\n",
    "        list2 = [(x,y) for x in self.patent_list for y in self.patent_list \\\n",
    "                 if type(x.patent_attributes[cited_fam]) ==list \\\n",
    "                 if y.patent_attributes[fam] in x.patent_attributes[cited_fam]]\n",
    "        \n",
    "        # Concatenating the two lists to have the direct citations\n",
    "        self.direct_citations = list1 + list2\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _get_BC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) Bibliographic coupling occurs when two works reference a common third work\n",
    "        # (2) The produced list is non directed.\n",
    "        # (3) Can be optimised\n",
    "        \"\"\"\n",
    "            \n",
    "        # Definition of variables\n",
    "        BC = []\n",
    "        a = self.patent_list\n",
    "        all_patent_pairs = [(a[p1], a[p2]) for p1 in range(len(a)) for p2 in range(p1+1,len(a))]\n",
    "\n",
    "        # Computing BC by looping over all pairs of patents\n",
    "        for patent_1, patent_2 in all_patent_pairs:\n",
    "            list_citing_1 = patent_1.patent_attributes[Config.NEW_VAR_CITING_DOCDB_FAM_IDS]\n",
    "            list_citing_2 = patent_2.patent_attributes[Config.NEW_VAR_CITING_DOCDB_FAM_IDS]\n",
    "            common_elements = [x for x in list_citing_1 if x in list_citing_2]\n",
    "            if len(common_elements)>0:\n",
    "                BC.append((patent_1, patent_2))\n",
    "            \n",
    "        # Removing duplicated items in the list\n",
    "        self.BC = list(set(BC)) \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _get_CC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) Co-citation is defined as the frequency with which two documents are cited together\n",
    "        by other documents. If at least one other document cites two documents in common these documents\n",
    "        are said to be co-cited\n",
    "        # (2) The produced list is non directed\n",
    "        \"\"\"\n",
    "        CC = []\n",
    "            \n",
    "        # Definition of all patent pairs\n",
    "        a = self.patent_list\n",
    "        all_patent_pairs = [(a[p1], a[p2]) for p1 in range(len(a)) for p2 in range(p1+1,len(a))]\n",
    "            \n",
    "        # Definition of the search algorithm\n",
    "        for patent in self.patent_list:\n",
    "            a = patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "            if type(a)==list:\n",
    "                if len(a)>1:\n",
    "                    all_cited_patent_pairs = [(a[p1], a[p2]) \\\n",
    "                                              for p1 in range(len(a)) \\\n",
    "                                              for p2 in range(p1+1,len(a))]\n",
    "                    for pair in all_cited_patent_pairs:\n",
    "                        CC.append(pair)\n",
    "        \n",
    "        pairs = list(set(CC)) \n",
    "        \n",
    "        CC = []\n",
    "        for pair in pairs:\n",
    "            patent1 = [patent \\\n",
    "                       for patent in self.patent_list \\\n",
    "                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] == pair[0]]\n",
    "            patent2 = [patent \\\n",
    "                       for patent in self.patent_list \\\n",
    "                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] == pair[1]]\n",
    "\n",
    "            if len(patent1)>0:\n",
    "                patent1 = patent1[0]\n",
    "            else: patent1=np.nan\n",
    "\n",
    "            if len(patent2)>0:\n",
    "                patent2 = patent2[0]\n",
    "            else: patent2=np.nan\n",
    "\n",
    "            pair = (patent1, patent2)\n",
    "            CC.append(pair)\n",
    "\n",
    "        self.CC = [pair for pair in CC if (pair[0]==pair[0]) & (pair[1] == pair[1])]\n",
    "        return self\n",
    "     \n",
    "        \n",
    "    def _get_LC_citations(self):\n",
    "        \"\"\"\n",
    "        # (1) LC (longitudinal coupling). A cites a document that cites B\n",
    "        # (2) The produced list IS directed \n",
    "        # (3) Can be optimised\n",
    "        \"\"\"          \n",
    "        LC = []\n",
    "            \n",
    "        # Identifying all patents cited by a given patent A\n",
    "        for patent_A in self.patent_list:\n",
    "            cited_fam = patent_A.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "            if type(cited_fam)==float:\n",
    "                    cited_fam = []\n",
    "                    cited_fam.append(patent_A.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID])\n",
    "            cited_patents = [patent \\\n",
    "                             for patent in self.patent_list \\\n",
    "                             if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] in cited_fam]\n",
    "                \n",
    "            # Identifying all patents cited by a patent cited by patent A\n",
    "            for cited_patent in cited_patents:\n",
    "                cited_fam = cited_patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID]\n",
    "                if type(cited_fam)==float:\n",
    "                    cited_fam = []\n",
    "                    cited_fam.append(cited_patent.patent_attributes[Config.VAR_CITED_DOCDB_FAM_ID])\n",
    "                cited_cited_patents = [patent \\\n",
    "                                       for patent in self.patent_list \\\n",
    "                                       if patent.patent_attributes[Config.VAR_DOCDC_FAMILY_ID] in cited_fam]\n",
    "                    \n",
    "                # Adding the pairs in the LC list\n",
    "                for patent_B in cited_cited_patents:\n",
    "                    LC.append((patent_A, patent_B))\n",
    "                    \n",
    "        # Removing duplicated items in the LC list\n",
    "        self.LC = list(set(LC)) \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class RetrieveFullTextData:\n",
    "    \"\"\"\n",
    "    Methods to retrieve the full text data to the selected patents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _assign_full_text_to_patents(self):\n",
    "        \"\"\"\n",
    "        For each patent contained in the model, assigns in the patent attributes\n",
    "        under the 'full_text' entry a dataframe containing all the text of this \n",
    "        patent, in raw format\n",
    "        \"\"\"\n",
    "        \n",
    "        lista = [str(x) for x in list(self.data['_text_data']['publication_number'])]\n",
    "        df = self.data['_text_data']\n",
    "        cols = list(df)\n",
    "\n",
    "        for patent in self.patent_list:\n",
    "            if patent.patent_attributes['publn_nr'] in lista:\n",
    "                data = df[df['publication_number'] == int(patent.patent_attributes['publn_nr'])]\n",
    "                patent.patent_attributes['full_text'] = data\n",
    "            else:\n",
    "                patent.patent_attributes['full_text'] = pd.DataFrame(columns=cols)\n",
    "        return self\n",
    "           \n",
    "        \n",
    "    @staticmethod    \n",
    "    def _translate(text_to_translate, source_language = False, target_language = 'en'):\n",
    "        \"\"\"Machine translation using the Google translate API\"\"\"\n",
    "\n",
    "        # initialise the translator object (see the googletrans API docs)\n",
    "        translator = Translator()\n",
    "        # behaviour of the function if the input language is known\n",
    "        if source_language != \"False\":\n",
    "            translatedText = translator.translate(text_to_translate, src=source_language, dest=target_language)\n",
    "        # if the language of the input is not known\n",
    "        else: \n",
    "            translatedText = translator.translate(text_to_translate, dest=target_language)\n",
    "        return translatedText.text\n",
    "\n",
    "    \n",
    "    @staticmethod \n",
    "    def _get_text_claims(data_patent_text):\n",
    "        \"\"\"For a given dataframe coming from the EP full-text database, returns\n",
    "        a list of its claim\"\"\"\n",
    "\n",
    "        ## data manipulation\n",
    "\n",
    "        # nickname \n",
    "        data = data_patent_text\n",
    "        # select only the claims in the dataframe\n",
    "        data = data[data['text_type']=='CLAIM']\n",
    "        # sort by data\n",
    "        data = data.sort_values(by = 'publication_date', ascending = False)\n",
    "        # keep only most recent claims by language\n",
    "        data.drop_duplicates(subset = ['language_text_component','text_type'], inplace = True)\n",
    "        # keep languages according to the other EN, DE, FR, other ('xx') (for best consistency of the translating)\n",
    "        data['language_text_component'] = pd.Categorical(data['language_text_component'] , categories=[\"en\",\"de\",\"fr\",\"xx\"], ordered=True)\n",
    "        data = data.sort_values(by = 'language_text_component')\n",
    "        data.drop_duplicates(subset = ['text_type'], inplace = True)\n",
    "\n",
    "        # if the data contains no claims then the data selected is empty\n",
    "        if len(data)==0:\n",
    "            return ['Unavailable']\n",
    "\n",
    "        else:\n",
    "\n",
    "            # store the language at this point for translation at the end\n",
    "            language = data.iloc[0]['language_text_component']\n",
    "            # selection of the field of the pandas dataframe which contains the claims texts\n",
    "            text_xml = data.iloc[0]['text']\n",
    "\n",
    "            ## Process the xml to get the raw text\n",
    "            \n",
    "            # removing the tags for bold text\n",
    "            text_xml_modified = text_xml.replace('<b>', '')\n",
    "            text_xml_modified = text_xml_modified.replace('</b>', '')\n",
    "\n",
    "            # modifying the claim to be processed as a real xml\n",
    "            text_xml_modified = \"<data>\" + text_xml_modified + '</data>'\n",
    "            # we parse it with the ElementTree XML APIÂ¶\n",
    "            root = ET.fromstring(text_xml_modified)\n",
    "            # and this is how we access the text of the claims\n",
    "            claims = root.findall(\"./claim/claim-text\")\n",
    "            # we store the claims in a list\n",
    "            claims_text = [claim.text for claim in claims]\n",
    "\n",
    "            ## Translate if the claims are not in EN\n",
    "            if language =='en':\n",
    "                pass\n",
    "            elif language == 'de':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, 'de', 'en') for text in claims_text]\n",
    "            elif language == 'fr':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, 'fr', 'en') for text in claims_text]\n",
    "            elif language == 'xx':\n",
    "                claims_text = [RetrieveFullTextData.translate(text, False, 'en') for text in claims_text]\n",
    "            else:\n",
    "                claims_text = [RetrieveFullTextData.translate(text, False, 'en') for text in claims_text]\n",
    "\n",
    "            return claims_text\n",
    "\n",
    "\n",
    "    def _attribute_claims(self):\n",
    "        \"\"\"For each patent, uses the _get_text_claims function to store the claims text in the object\"\"\"\n",
    "        for patent in self.patent_list:\n",
    "            patent.patent_attributes['full_text_claims'] = RetrieveFullTextData._get_text_claims(patent.patent_attributes['full_text'])\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CustomStemmer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"We overwrite the Sklearn BaseEstimator class in order to have more control on the \n",
    "    text data preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, stemmer_type):\n",
    "        \"\"\"We can use different types of stemmer\"\"\"\n",
    "        \n",
    "        self.tokenizer = RegexpTokenizer(\"(?u)\\\\b[\\\\w-]+\\\\b\")\n",
    "        self.stemmer_type = stemmer_type\n",
    "        \n",
    "        if stemmer_type == 'snowball':\n",
    "            self.stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        \"\"\"Overwritten for the sake of completeness, does not perform any action\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Returns a stemmed version of the documents, using the Porter algorithm (snowball)\n",
    "        and removing English stop words\"\"\"\n",
    "        \n",
    "        if self.stemmer_type!='no':\n",
    "            documents = documents.apply(self.tokenizer.tokenize)\n",
    "            # apply the second tokenizer here\n",
    "\n",
    "            def snowball(x):\n",
    "                liste = [self.stemmer.stem(y) for y in x]\n",
    "                return liste\n",
    "\n",
    "            # stemming\n",
    "            documents = documents.apply(snowball)\n",
    "            \n",
    "            # remove stop words\n",
    "            nltk.download('stopwords')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # stopset = set(stopwords.words('english'))\n",
    "            \n",
    "            stopwords = nltk.corpus.stopwords.words('english')\n",
    "            stopwords.append('claim')\n",
    "            stopwords.append('according')\n",
    "            stopwords.append('preceding')\n",
    "            stopwords.append('characterised')\n",
    "            stopset = set(stopwords)\n",
    "            \n",
    "            def remove_stop_words(x):\n",
    "                liste = [y for y in x if not y in stopset]\n",
    "                return liste\n",
    "            \n",
    "            documents = documents.apply(remove_stop_words)\n",
    "            \n",
    "            def remove_numbers(x):\n",
    "                liste = [y for y in x if not any(char.isdigit() for char in y)]\n",
    "                return liste\n",
    "            \n",
    "            documents = documents.apply(remove_numbers)\n",
    "\n",
    "            # join\n",
    "            documents = [' '.join(docs) for docs in documents]\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CustomVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"We overwrite the Sklearn BaseEstimator class in order to have more control on the vectorisation\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_type):\n",
    "        \"\"\"Two possibilities here: count and tfidf. More can be added if neccessary\"\"\"\n",
    "        \n",
    "        if vectorizer_type == 'count':\n",
    "            self.vectorizer = CountVectorizer(binary=True)\n",
    "    \n",
    "        if vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        \"\"\"Does not perform any action\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Return a numpy arraw - the feature space\"\"\"\n",
    "        freqs = self.vectorizer.fit_transform(documents)\n",
    "        return [freq.toarray()[0] for freq in freqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TextProcessing:\n",
    "    \"\"\"\n",
    "    Methods for text analysis and similarity measures, using the 2 classes above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _index_patents(self):\n",
    "        \"\"\"We create an index of patents, in order to be able to process all the text together in\n",
    "        the following text processing steps and still be able to access individual patent text data\"\"\"\n",
    "        \n",
    "        self.dict_patents_indexes = {k: v for v, k in enumerate(self.patent_list)}\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _store_vocabulary(self):\n",
    "        \"\"\"Store all the vocabulary contained in the patents in a Panda series called 'corpus' \"\"\"\n",
    "        \n",
    "        # create an empty list and fill it with the claim text of each patent in a separate entry\n",
    "        l = []\n",
    "        for patent in self.patent_list:\n",
    "            l.append(patent.patent_attributes['full_text_claims'])\n",
    "        # flatten the nested list by joining the sentences/claims together in a single sentence\n",
    "        # for each patent / except if the text is unavailable\n",
    "        l = [' '.join(element) for element in l if isinstance(element, str)==False]\n",
    "        # reshape as a single Pandas serie and store in the corpus\n",
    "        self.corpus = pd.DataFrame(l, columns=['text']).pop('text')\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _stemming(self):\n",
    "        \"\"\"Reducing words to their stem word (semantic root), and remove the English stop words\"\"\"\n",
    "        \n",
    "        stemmer = CustomStemmer('snowball')\n",
    "        self.corpus_stemmed = stemmer.transform(self.corpus)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _vectorize(self):\n",
    "        \"\"\"Vectorise the patents in a high dimention space (returns a list)\"\"\"\n",
    "        \n",
    "        custom_vectorizer = CustomVectorizer('tfidf')\n",
    "        self.feature_space = custom_vectorizer.fit_transform(self.corpus_stemmed)\n",
    "        \n",
    "        # iterating through all pairs of patents to get their pairwise similarity\n",
    "        for patent in model.patent_list:\n",
    "            index = model.dict_patents_indexes[patent]\n",
    "            patent.patent_attributes['stemmed text'] = model.corpus_stemmed[index]\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def _compute_pairwise_similarities(self):\n",
    "        \"\"\"returns a numpy.ndarray containing all pairwise similarities between patents\"\"\"\n",
    "        \n",
    "        # https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\n",
    "        # To compute the cosine distance of the first doc to all the others \n",
    "        from sklearn.metrics.pairwise import linear_kernel\n",
    "        # in this case linear_kernel is equivalent to cosine_similarity because the TfidfVectorizer produces normalized vectors.\n",
    "        # returs an array with all pairwise similarities!\n",
    "        self.cosine_similarities = linear_kernel(self.feature_space, self.feature_space)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _similarity(self, patent1, patent2):\n",
    "        \"\"\"Measure the similiarity between a pair of linked patents pair = (patent1, patent2)\"\"\"\n",
    "        i = self.dict_patents_indexes[patent1]\n",
    "        j = self.dict_patents_indexes[patent2]\n",
    "        return self.cosine_similarities[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BuildNetwork():\n",
    "    \"\"\"Builds a weighted network based on backwards citations and text similarity\"\"\"\n",
    "    \n",
    "    def _create_network(self):\n",
    "        \"\"\"Create the weighted and undirected network with igraph\"\"\"\n",
    "\n",
    "        def filter_symmetric_duplicates(l:list):\n",
    "            \"\"\"Code snippet to filter symmetric duplicates in a list of tuples\n",
    "            Eg [(1,2), (2,1)] -> [(1,2)]\"\"\"\n",
    "            seen = []\n",
    "            for pair in l:\n",
    "                if pair in seen:\n",
    "                    l.remove(pair)\n",
    "                seen.append(tuple(reversed(pair)))\n",
    "            return l\n",
    "        \n",
    "        # defining all possible links between any pair of patents\n",
    "        links = self.direct_citations + self.CC + self.BC + self.LC\n",
    "        \n",
    "        # definition of the links\n",
    "        links = filter_symmetric_duplicates(links)\n",
    "        weighted_links = [(p1, p2, TextProcessing._similarity(self, p1, p2)) for (p1, p2) in links]\n",
    "        \n",
    "        # creation of the graph\n",
    "        self.graph = Graph.TupleList(weighted_links, weights=True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def _simplify_network(self):\n",
    "        \"\"\" \n",
    "        Removing (1) multiple edges (i.e. several links between patents) and (2) loops (i.e. links\n",
    "        between a patent and itself).\n",
    "        \n",
    "        When multiple edges are removed, they are replaced by a single edge with the weight\n",
    "        of the maximum weight of the previsous edges (normally all equal, since it is a \n",
    "        similarity measures of the nodes).\n",
    "        \"\"\"\n",
    "        self.graph = self.graph.simplify(multiple=True, loops=True, combine_edges=max)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _select_resolution_parameter(self):\n",
    "        \"\"\"\n",
    "        Selecting the resolution parameter which maximise the modularity of the graph with the\n",
    "        Leiden algorithm:\n",
    "        # show the table of values\n",
    "        # display the figure showing the result\n",
    "        \"\"\"\n",
    "    \n",
    "        # looping over a wide range of possible resolution parameters to find the one\n",
    "        # which maximise the modularity of the graph (more or less the goodness of the fit\n",
    "        # of the partition of the graph in communitities/clusters)\n",
    "        l = []\n",
    "        for i in [x * 0.01 for x in range(0, 350)]:\n",
    "            resolution_parameter = i\n",
    "            comms = model.graph.community_leiden(objective_function = 'modularity',\n",
    "                                                 n_iterations = -1,\n",
    "                                                 weights = model.graph.es['weight'],\n",
    "                                                 resolution_parameter=resolution_parameter)\n",
    "            l.append({'resolution_parameter':i,\n",
    "                      'modularity':model.graph.modularity(comms),\n",
    "                     'nb_clusters':len(comms)})\n",
    "        df = pd.DataFrame(l)\n",
    "        # selection of the best performing resolution parameter and storing\n",
    "        # it has an attribute of the model 'best_resolution_parameter'\n",
    "        self.best_resolution_parameter = df.sort_values(by = 'modularity', ascending = False).reset_index(drop=True)['resolution_parameter'][0]\n",
    "        \n",
    "        # plotting the figure showing the selection\n",
    "        \n",
    "        # size\n",
    "        sns.set(rc={'figure.figsize':(12,4)})\n",
    "        # plotting aesthetics\n",
    "        sns.set_style('white')\n",
    "        # plot the line for modularity\n",
    "        ax = df.plot(x=\"resolution_parameter\", y=\"modularity\", legend=False)\n",
    "        # on a second axis, plot the line for the resolution parameter\n",
    "        ax2 = ax.twinx()\n",
    "        df.plot(x=\"resolution_parameter\", y=\"nb_clusters\", ax=ax2, legend=False, color=\"r\")\n",
    "        # legend\n",
    "        ax.figure.legend(loc='bottom left')\n",
    "        # display\n",
    "        plt.show()\n",
    "        # display the table of results\n",
    "        display(df.sort_values(by = 'modularity', ascending = False).head(20))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def _fit_Leiden_clustering_algorithm(self):\n",
    "        \"\"\"Get the community structure of the graph using the Leiden clustering algorithm\n",
    "        defined in https://www.nature.com/articles/s41598-019-41695-z\"\"\"\n",
    "        \n",
    "        self.community_structure = model.graph.community_leiden(\n",
    "            # the objective function is the graph modularity\n",
    "            # https://en.wikipedia.org/wiki/Modularity_(networks)\n",
    "            objective_function = 'modularity',\n",
    "            # the algorithm iterate until convergence (coded by -1 steps in the API)\n",
    "            n_iterations = -1,\n",
    "            # the weights of the edges are the ones computed by the text similarity metric\n",
    "            weights = model.graph.es['weight'],\n",
    "            # the resolution parameters is the one previously computed, which \n",
    "            # maximise the modularity of the community structure (with iterations until\n",
    "            # covergence as well)\n",
    "            resolution_parameter = self.best_resolution_parameter\n",
    "            )\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SummaryStatistics:\n",
    "    \"\"\"Summary statistics for the data section\"\"\"\n",
    "    # Can also help comparing before data cleaning and after!\n",
    "    \n",
    "    def _print_nb_patents(self):\n",
    "        \"\"\"Printing info\"\"\"\n",
    "        print('..Nb of patents:',len(self.data['_table_main_patent_infos']\\\n",
    "                                     [Config.VAR_APPLN_ID].unique().tolist()))\n",
    "        \n",
    "        \n",
    "    def _plot_patent_filling_over_time(self):\n",
    "        \"\"\"Display a plot with the patent filling over time\"\"\"\n",
    "        \n",
    "        l = []\n",
    "        for patent in self.patent_list:\n",
    "            l.append(patent.patent_attributes['appln_filing_year'])\n",
    "        df = pd.DataFrame(l,columns=['appln_filing_year'])['appln_filing_year'].value_counts().to_frame().reset_index()\n",
    "        df.columns = ['appln_filing_year', 'count']\n",
    "        df.sort_values(inplace = True, by = 'appln_filing_year')\n",
    "        sns.lineplot(data=df,\n",
    "                     x=\"appln_filing_year\",\n",
    "                     y= 'count').set_title('Patent filling over time', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Visualisation:\n",
    "    \"\"\"Visualisation methods\"\"\"\n",
    "    \n",
    "    def _draw_graph_with_communities(self, kind = 'leiden', nb_iterations = -1):\n",
    "        \"\"\"Plot the graph with a custom layout and showing the communities:\n",
    "        - kind = 'louvain' display the result according to the Louvain algorithm \n",
    "        by Blondel et al.\n",
    "        - kind = 'leiden' display the result according to the Leiden algorithm\n",
    "        - nb_iterations = -1 iterates until convergence.\n",
    "        \"\"\"\n",
    "        \n",
    "        # defining the visual layout for the graph\n",
    "        visual_style = {}\n",
    "        # size of the vertex (nodes)\n",
    "        visual_style[\"vertex_size\"] = 5\n",
    "        # the thickness of the edges is proportionnal to the strenght of the link between the patents\n",
    "        visual_style[\"edge_width\"] = [int(2 * weight)+0.01 for weight in model.graph.es['weight']]\n",
    "        \n",
    "        if kind == 'louvain':\n",
    "            # LOUVAIN: https://igraph.org/r/doc/cluster_louvain.html\n",
    "            # this fonction is implemented in C in igraph and called 'community_multilevel'\n",
    "            print('Using the Louvain algorithm:')\n",
    "            comms = self.graph.community_multilevel(weights = self.graph.es['weight'])\n",
    "            # plotting the graph with the communities and the visual style\n",
    "            display(igraph.plot(comms, mark_groups = True, **visual_style))\n",
    "        \n",
    "        if kind == 'leiden':\n",
    "            \n",
    "            if nb_iterations!=-1:\n",
    "                # LEIDEN: https://www.nature.com/articles/s41598-019-41695-z\n",
    "                print('Using the Leiden algorithm, {} iterations:'.format(nb_iterations))\n",
    "                comms = model.graph.community_leiden(objective_function = 'modularity',\n",
    "                                                     weights = model.graph.es['weight'],\n",
    "                                                    resolution_parameter = self.best_resolution_parameter)\n",
    "                # plotting the graph with the communities and the visual style\n",
    "                display(igraph.plot(comms, mark_groups = True, **visual_style))\n",
    "\n",
    "            else:\n",
    "                print('Using the Leiden algorithm, iterating until convergence:')\n",
    "                print('This is the final model!')\n",
    "\n",
    "                # nickname for clarity\n",
    "                community_structure = self.community_structure\n",
    "\n",
    "                # get the number of clusters and store the value in the model\n",
    "                self.nb_clusters = len(self.community_structure)\n",
    "                # defining all the range of possible colors for clusters from matplotlib colors\n",
    "                colors = ['blue','paleturquoise','green','gold','red','grey','fuchsia', 'black', 'ivory',\n",
    "                         'firebrick','lime','bisque','lightgrey']\n",
    "                # creation of a color palette according to the number of clusters\n",
    "                colors_selected = colors[:self.nb_clusters]\n",
    "                pal = igraph.PrecalculatedPalette(colors_selected)\n",
    "\n",
    "                # setting the random seed so that to fix the output\n",
    "                random.seed(1234)\n",
    "                # plotting the graph with the communities and the visual style\n",
    "                # without marking communities\n",
    "                display(igraph.plot(community_structure,\n",
    "                                    palette = pal,\n",
    "                                    **visual_style))\n",
    "\n",
    "                # plot the legend\n",
    "                legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster '+ str(i+1),\n",
    "                                          markerfacecolor=c, markersize=15) for i,c in enumerate(colors_selected)]\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.legend(handles=legend_elements, loc='center')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                # setting the random seed so that to fix the output\n",
    "                random.seed(1234)\n",
    "                # plotting the graph with the communities and the visual style\n",
    "                # with marking of the communities with polygons\n",
    "                display(igraph.plot(community_structure,\n",
    "                                    palette = pal,\n",
    "                                    mark_groups = True,\n",
    "                                    **visual_style))\n",
    "                # plot the legend again\n",
    "                legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster '+ str(i+1),\n",
    "                                          markerfacecolor=c, markersize=15) for i,c in enumerate(colors_selected)]\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.legend(handles=legend_elements, loc='center')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def normalise_voc(voc_cluster, voc_corpus):\n",
    "\n",
    "        def vectorise_voc(data):\n",
    "            \"\"\"For a given corpus, compute the frequency of the words\"\"\"\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            vecs = vectorizer.fit_transform(data)\n",
    "            feature_names = vectorizer.get_feature_names()\n",
    "            dense = vecs.todense()\n",
    "            lst1 = dense.tolist()\n",
    "            df = pd.DataFrame(lst1, columns=feature_names)\n",
    "            data = df.T.sum(axis=1)\n",
    "            data = data.sort_values(ascending = False)\n",
    "            return data\n",
    "\n",
    "        # create a table to compare the frequency of a term inside the cluster \n",
    "        # and in the total vocabulary\n",
    "        df1 = vectorise_voc(voc_corpus).to_frame()\n",
    "        df1.columns = ['freq_corpus']\n",
    "        df2 = vectorise_voc(voc_cluster).to_frame()\n",
    "        df2.columns = ['freq_cluster']\n",
    "        result_df = pd.concat([df1, df2], axis=1, sort=False)\n",
    "\n",
    "        # floor to eliminate corner cases \n",
    "        result_df = result_df[result_df['freq_cluster']>=0.1]\n",
    "\n",
    "        # create a new variable to select the most decisive words for each cluster\n",
    "        # = squared frequency in the cluster / frequency in the total vocabulary\n",
    "        result_df['squared_freq_cluster_div_freq_corpus'] = result_df['freq_cluster']*result_df['freq_cluster']/result_df['freq_corpus']\n",
    "        result_df = result_df.sort_values(by = 'squared_freq_cluster_div_freq_corpus', ascending = False)\n",
    "        # show the table\n",
    "        display(result_df.head(10))\n",
    "\n",
    "        return result_df['squared_freq_cluster_div_freq_corpus']\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def show_wordcloud(voc_cluster,voc_corpus):\n",
    "        \"\"\"Create a WordCloud\"\"\"\n",
    "\n",
    "        data = Visualisation.normalise_voc(voc_cluster, voc_corpus)\n",
    "        Cloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(data)\n",
    "\n",
    "        # controlling figure aesthetics\n",
    "        fig = plt.figure(1, figsize=(12, 12))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(Cloud)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _display_cluster_word_clouds(self):\n",
    "        \"\"\"Compute for each cluster the most original words in contains, \n",
    "        and display the result in a WordCloud\"\"\"\n",
    "\n",
    "        # the community structure\n",
    "        comms = self.community_structure\n",
    "\n",
    "        # looping over the clusters\n",
    "        for i in range(0,len(comms)):\n",
    "            print('Cluster {}:'.format(i+1))\n",
    "\n",
    "            # retrieving all patents belonging to the given cluster\n",
    "            ids_patents = comms[i]\n",
    "            # adding the stemmed corpus of all patent to get the cluster specific corpus\n",
    "            l = []\n",
    "            for id_patents in ids_patents:\n",
    "                text_stemmed = comms.graph.vs[id_patents]['name'].patent_attributes['stemmed text']\n",
    "                l.append(text_stemmed)\n",
    "            voc_cluster = l\n",
    "            # showing the wordcloud, once compared the corpus of the cluster with the \n",
    "            # corpus of the entire model (all patents)\n",
    "            Visualisation.show_wordcloud(voc_cluster, voc_corpus = self.corpus_stemmed)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _display_S_curves(self):\n",
    "        \"\"\"Show the propagation of the different technologies\"\"\"\n",
    "\n",
    "        # the community structure\n",
    "        comms = self.community_structure\n",
    "\n",
    "        # looping over the clusters\n",
    "        dfs = []\n",
    "        for i in range(0,len(comms)):\n",
    "\n",
    "            # retrieving all patents belonging to the given cluster\n",
    "            ids_patents = comms[i]\n",
    "            # adding the application filling year for each patent i the cluster\n",
    "            l = []\n",
    "            for id_patents in ids_patents:\n",
    "                date = comms.graph.vs[id_patents]['name'].patent_attributes['appln_filing_year']\n",
    "                l.append(date)\n",
    "            dates = l\n",
    "            df = pd.DataFrame(l,columns=['appln_filing_year'])\n",
    "            df['cluster'] = 'Cluster {}'.format(i+1)\n",
    "            # showing the wordcloud, once compared the corpus of the cluster with the \n",
    "            # corpus of the entire model (all patents)\n",
    "            dfs.append(df)\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "        l = []\n",
    "        for cluster in df['cluster'].unique().tolist():\n",
    "            for year in df['appln_filing_year'].unique().tolist():\n",
    "\n",
    "                condition1 = df['appln_filing_year']<=year\n",
    "                condition2 = df['cluster']==cluster\n",
    "                nb = len(df[condition1 & condition2])\n",
    "\n",
    "                l.append([cluster, year, nb])\n",
    "        df = pd.DataFrame(l,columns=['Cluster:', 'appln_filing_year','count'])\n",
    "        sns.lineplot(data=df,\n",
    "                     x=\"appln_filing_year\",\n",
    "                     y= 'count',\n",
    "                     hue=\"Cluster:\",\n",
    "                     style=\"Cluster:\",\n",
    "                     markers=True,\n",
    "                     dashes=False).set_title('S-curves for the different technologies identified',\n",
    "                                             fontsize = 14);\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     28,
     33,
     38,
     48,
     54,
     62,
     69,
     85,
     93,
     99
    ]
   },
   "outputs": [],
   "source": [
    "class Model(Config, DataCleaning, NewMetrics, ReshapingToOOP, GetCitations, RetrieveFullTextData,\n",
    "            TextProcessing, BuildNetwork, SummaryStatistics, Visualisation):\n",
    "    \"\"\"\n",
    "    Creation of a model which inherits several building blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attributes of the model\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data: dict # datasets\n",
    "        patent_list: list # patent objects\n",
    "        dict_patents_indexes: dict # mapping of patents objects and their indexes\n",
    "        patent_ids: list # list of patent ids contained in the model\n",
    "        patent_family_ids: list # list of DOCDB family ids contained in the model\n",
    "        direct_citations: list # directed list of simple citations\n",
    "        CC: list # undirected list of co-citations\n",
    "        BC: list # undirected list of bibliographical coupling\n",
    "        LC: list # directed list of longitudinal citations\n",
    "        corpus: pandas.core.series.Series # contains all claim text of each patent (raw)\n",
    "        corpus_stemmed: pandas.core.series.Series # contains all claim text of each patent (stemmed)\n",
    "        feature_space: list # the feature space, the high dimension representation of the text data\n",
    "        cosine_similarities: numpy.ndarray # contains all the pairwise similarities between patents\n",
    "        graph: igraph.Graph # Igraph network \n",
    "        best_resolution_parameter: int # best resolution parameter for the Leiden algorithm\n",
    "        community_structure: igraph.clustering.VertexClustering # community structure identified\n",
    "        \n",
    "    \n",
    "    def _input_data(self, data):\n",
    "        \"\"\"Getting the data in the model\"\"\"\n",
    "        self.data = data\n",
    "    \n",
    "    \n",
    "    def _compute_new_metrics(self):\n",
    "        \"\"\"Adding new variables in the dataset\"\"\"\n",
    "        self = NewMetrics._get_DOCDB_fam_cites_per_year(self)  \n",
    "    \n",
    "    \n",
    "    def _data_cleaning(self):\n",
    "        \"\"\"Data cleaning using the DataCleaning class methods\"\"\"\n",
    "        self = DataCleaning._keep_only_EP_patents(self)\n",
    "        self = DataCleaning._keep_only_granted_patents(self)\n",
    "        self = DataCleaning._select_time_range(self)\n",
    "        #self = DataCleaning._normalise(self)\n",
    "        self = DataCleaning._select_one_patent_per_family(self)\n",
    "        self = DataCleaning._select_breakthrough_patents(self)\n",
    "    \n",
    "    \n",
    "    def _fit_to_object_oriented_design(self):\n",
    "        \"\"\"We reshape the data from a tabular form to an object oriented form\"\"\"\n",
    "        self = ReshapingToOOP._create_patent_objects(self)\n",
    "        self = ReshapingToOOP._assign_data_to_patent_obj(self)  \n",
    "   \n",
    "\n",
    "    def _get_citations(self):\n",
    "        \"\"\"Identify direct and indirect citations that link the patents\"\"\"\n",
    "        self = GetCitations._get_direct_citations(self)\n",
    "        self = GetCitations._get_CC_citations(self)\n",
    "        self = GetCitations._get_BC_citations(self)\n",
    "        self = GetCitations._get_LC_citations(self)\n",
    "        \n",
    "    \n",
    "    def _get_full_text(self):\n",
    "        \"\"\"Retrieve the full text data corresponding to the patents in the model, \n",
    "        extract the claims and attribute them to the patent objects\"\"\"\n",
    "        self = RetrieveFullTextData._assign_full_text_to_patents(self)\n",
    "        self = RetrieveFullTextData._attribute_claims(self)\n",
    "    \n",
    "    \n",
    "    def _text_preprocessing(self):\n",
    "        \"\"\"Computing text similarities between linked patents\n",
    "        \n",
    "        # 1. index all the patents in the 'dict_patents_indexes' dictionnary\n",
    "        # 2. get all the vocabulary stored in a single Pandas serie\n",
    "        # 3. text preprocessing\n",
    "        # 4. vectorisation and create the feature space \n",
    "        # 5. compute the array of all pairwise similarities from the feature space\n",
    "        \"\"\"\n",
    "        self = TextProcessing._index_patents(self)\n",
    "        self = TextProcessing._store_vocabulary(self)\n",
    "        self = TextProcessing._stemming(self)\n",
    "        self = TextProcessing._vectorize(self)\n",
    "        self = TextProcessing._compute_pairwise_similarities(self)\n",
    "     \n",
    "        \n",
    "    def _build_patent_network(self):\n",
    "        \"\"\"We build the patent network (weighted directed graph)\"\"\"\n",
    "        self = BuildNetwork._create_network(self)\n",
    "        self = BuildNetwork._simplify_network(self)\n",
    "        self = BuildNetwork._select_resolution_parameter(self)\n",
    "        self = BuildNetwork._fit_Leiden_clustering_algorithm(self)\n",
    "        \n",
    "    \n",
    "    def _display_summary_statistics(self):\n",
    "        \"\"\"Summary statistics about the model created and its data\"\"\"\n",
    "        SummaryStatistics._print_nb_patents(self)\n",
    "        SummaryStatistics._plot_patent_filling_over_time(self)\n",
    "    \n",
    "    \n",
    "    def _visualise(self):\n",
    "        \"\"\"Show the different visualisations\"\"\"\n",
    "        self = Visualisation._draw_graph_with_communities(self)\n",
    "        self = Visualisation._display_cluster_word_clouds(self)\n",
    "        self = Visualisation._display_S_curves(self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
