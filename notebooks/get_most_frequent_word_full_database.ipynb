{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the English vocabulary of the EP full-text data for text analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antoine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# librairies\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# library to parse the xml content of the EP full text database\n",
    "# library doc: https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# language processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# location the the files - EP full text data 2020 edition\n",
    "fpattern = r'../data/ep_full_text_database/2020_edition/EP{}.txt'\n",
    "files = glob.glob(fpattern.format('*','*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# config\n",
    "sep = '\\t'\n",
    "text_type = 'CLAIM'\n",
    "lang = 'en'\n",
    "\n",
    "new_col_names = ['publication_authority', # will always have the value \"EP\"\n",
    "                 'publication_number', # a seven-digit number\n",
    "                 'publication_kind', # see https://www.epo.org/searching-for-patents/helpful-resources/first-time-here/definitions.html for help.\n",
    "                 'publication_date', # in format YYYY-MM-DD\n",
    "                 'language_text_component', # de, en, fr; xx means unknown\n",
    "                 'text_type', # TITLE, ABSTR, DESCR, CLAIM, AMEND, ACSTM, SREPT, PDFEP\n",
    "                 'text' # it contains, where appropriate, XML tags for better structure. You will find the DTD applicable to all parts of the publication at: http://docs.epoline.org/ebd/doc/ep-patent-document-v1-5.dtd\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonction to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_claim_text(f):\n",
    "    \"\"\"Open the file f to get all the claims in English\"\"\"\n",
    "    \n",
    "    # reading the file \n",
    "    print('Reading the file: {}'.format(f))\n",
    "    df = pd.read_csv(f, sep = sep)\n",
    "    \n",
    "    # changing the column names\n",
    "    df.columns = new_col_names\n",
    "    \n",
    "    # filtering to keep only claims in English (and only once)\n",
    "    condition1 = df['text_type'] == text_type\n",
    "    condition2 = df['language_text_component'] == lang\n",
    "    df.drop_duplicates(subset = ['text'], inplace = True)\n",
    "    df = df[condition1 & condition2]['text'].to_frame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parsing(text_xml):\n",
    "    \"\"\"Process the xml to get the raw text\"\"\"\n",
    "            \n",
    "    # removing the tags for bold text\n",
    "    text_xml_modified = text_xml.replace('<b>', '')\n",
    "    text_xml_modified = text_xml_modified.replace('</b>', '')\n",
    "\n",
    "    # modifying the claim to be processed as a real xml\n",
    "    text_xml_modified = \"<data>\" + text_xml_modified + '</data>'\n",
    "    # we parse it with the ElementTree XML APIÂ¶\n",
    "    root = ET.fromstring(text_xml_modified)\n",
    "    # and this is how we access the text of the claims\n",
    "    claims = root.findall(\"./claim/claim-text\")\n",
    "    # we store the claims in a list\n",
    "    claims_text = [claim.text for claim in claims]\n",
    "    \n",
    "    return claims_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2400000.txt\n",
      "CPU times: user 1min 13s, sys: 6.27 s, total: 1min 20s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test with a single file\n",
    "df = get_claim_text(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fetch_all_vocabulary(files):\n",
    "    l = []\n",
    "    for f in files:\n",
    "        text = get_claim_text(f)\n",
    "        text['text'].apply(parsing)\n",
    "        l.append(df)\n",
    "    return pd.concat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3100000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0800000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0300000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0500000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1900000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0700000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2200000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP1400000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP0000000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP3600000.txt\n",
      "Reading the file: ../data/ep_full_text_database/2020_edition/EP2100000.txt\n",
      "CPU times: user 47min 25s, sys: 1min 49s, total: 49min 14s\n",
      "Wall time: 1h 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = fetch_all_vocabulary(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# reshape \n",
    "documents = df['text'].apply(parsing)\n",
    "documents = documents.apply(lambda x:x[0])\n",
    "documents.dropna(inplace=True)\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer(\"(?u)\\\\b[\\\\w-]+\\\\b\")\n",
    "documents = documents.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     4,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def remove_capitalisation(x):\n",
    "    liste = [y.lower() for y in x]\n",
    "    return liste\n",
    "\n",
    "def remove_stop_words(x):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    liste = [y for y in x if not y in stopset]\n",
    "    return liste\n",
    "\n",
    "def remove_numbers(x):\n",
    "    liste = [y for y in x if not any(char.isdigit() for char in y)]\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents.apply(remove_capitalisation)\n",
    "documents = documents.apply(remove_stop_words)\n",
    "documents = documents.apply(remove_numbers)\n",
    "documents = documents.apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the most frequent words in the dataset\n",
    "Counter(\" \".join(documents).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
